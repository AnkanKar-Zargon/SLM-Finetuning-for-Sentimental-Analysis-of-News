# -*- coding: utf-8 -*-
"""Fine Tuning SmolLM-135M For News Sentiment Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c4LKEpCZsrB_DuhGivvIos0gpS5UI0hr
"""

# ================================
# Fine-Tune SmolLM-135M on News Category Dataset
# ================================

!pip install -q transformers datasets accelerate evaluate bitsandbytes pandas

import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from datasets import Dataset
import evaluate
from google.colab import drive

# ----------------
# 1. Load dataset
# ----------------
drive.mount('/content/drive')
data_path = '/content/drive/MyDrive/LLM & SLM/News_Category_Dataset_v3.json'
df = pd.read_json(data_path, lines=True)

# Keep only "headline" and "category"
df = df[["headline", "category"]]
print("Sample rows:\n", df.head())

# ----------------
# 2. Encode labels
# ----------------
label_list = sorted(df["category"].unique().tolist())
label2id = {label: i for i, label in enumerate(label_list)}
id2label = {i: label for label, i in label2id.items()}

df["label"] = df["category"].map(label2id)

# Train/test split
train_df = df.sample(frac=0.8, random_state=42)
test_df = df.drop(train_df.index)

# Subsample for Colab speed (optional)
train_df = train_df.sample(n=10000, random_state=42)   # 10k samples
test_df = test_df.sample(n=2000, random_state=42)     # 2k samples

train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)

# ----------------
# 3. Load tokenizer & model
# ----------------
model_id = "HuggingFaceTB/SmolLM-135M"
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Fix: Ensure tokenizer has a pad token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token  # reuse EOS as PAD

model = AutoModelForSequenceClassification.from_pretrained(
    model_id,
    num_labels=len(label_list),
    id2label=id2label,
    label2id=label2id
)

# Match model config with tokenizer pad token
model.config.pad_token_id = tokenizer.pad_token_id

# ----------------
# 4. Preprocess data
# ----------------
def tokenize_function(examples):
    return tokenizer(
        examples["headline"],
        padding="max_length",
        truncation=True,
        max_length=64
    )

train_dataset = train_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

# ----------------
# 5. Evaluation metric
# ----------------
accuracy = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = torch.argmax(torch.tensor(logits), dim=-1)
    return accuracy.compute(predictions=predictions, references=labels)

# ----------------
# 6. Training setup
# ----------------
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",   # âœ… correct arg name
    learning_rate=2e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    weight_decay=0.01,
    # lr_scheduler_type="cosine",
    logging_dir='./logs',
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    push_to_hub=False,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

# ----------------
# 7. Train
# ----------------
trainer.train()

# ----------------
# 8. Evaluate
# ----------------
results = trainer.evaluate()
print("Final evaluation:", results)

# ----------------
# 9. Save fine-tuned model to Drive
# ----------------
save_path = "/content/drive/MyDrive/LLM & SLM/smollm-news"
model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)
print(f"Model saved to {save_path}")

# ----------------
# 10. Test with a sample (inference)
# ----------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

text = "Stocks rally as Federal Reserve signals possible rate cuts."
inputs = tokenizer(text, return_tensors="pt").to(device)
outputs = model(**inputs)
prediction = torch.argmax(outputs.logits, dim=-1).item()
print("Predicted category:", id2label[prediction])